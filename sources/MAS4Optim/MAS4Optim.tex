\part{A Multi-Agent System for Continuous Optimization}

In the previous part we discussed an inherent limitation of the current continuous optimization approaches. Not only the current methods are highly specialized, but they are also limited by the size and ultimately by the complexity of the problem. The most complex problems require specific approaches (Multidisciplinary methods) in order to distribute this complexity while keeping a coherent view. However, these approaches are often complicated to put in practice and tend themselves to be specialized to certain problem types.

In this part we present a novel approach by modeling the problem to solve with a multi-agent system. The system then solves the problem in a distributed way while maintaining a coherent view of the problem by propagating messages from neighbors to neighbors. 

First we propose a way to model a continuous optimization  as a MAS. First of all we want our modeling must be general enough to allow any continuous optimization problem to be transform in such a way. We also want it simple enough to be automated, which is not only a requirement in the case of big problems, but also a way for the MAS to work \enquotes{behind the scenes}, without requiring the user of the system to have a specific knowledge of multi-agent related concepts. Since optimization specialists often possess expert  knowledge and techniques associated to the problems they want to solve, we would like our modeling to be able to integrate with external optimization tools. Our last requirement is the model to allow the person in charge of the optimization process to be able to express the problem in the formulation which is the most natural for him, without requiring the problem to be reformulated in any way.

After presenting our modeling, we expose the basic, nominal workflow of the system, how the agents try to optimize the different part of the system and exchanges inform and request messages in order to propagates changes and coordinate the optimization process. We then see how specific configurations related to complex continuous optimization problems can cause this nominal optimization workflow to fail. Using the AMAS theory, we categorize these configurations into different non cooperative situations, for each of which we will propose additional cooperative mechanisms in  order to detect, solve the NCS and re-establish the correct optimization process.

At last, we see how our agent modeling can be extended by proposing some modification in our system in order to be able to take in account the handling of uncertainties during the optimization process. We first see how the expression of the problem is changed by the integration of uncertainties. We modify the structure of the information exchanged by the agents and identify some key operation they require in order to be able to manipulate exchanged data. Finally we propose a generic mechanism for experts to be able to express how different representations of uncertainties can be transformed in the context of the problem to solve.

\chapter{Agent-Based Modeling and Simulation of a Continuous Optimization Problem}

[[Insister sur le découpage inhérent au problème (pas de réductionnisme)]]
[[utiliser un exemple en fil rouge -> Turbofan ?]]

\section{Problem Modeling with NDMO}\label{modeling}

[[A CHANGER: In answer to the previous shortcomings, we propose a generic approach called Natural Domain Modeling for Optimization (NDMO) that relies on a natural or intrinsic description of the problem (\textit{i.e.} close to the reality being described).]]

\begin{figure}[]
	\centering
	\includegraphics[width=0.8\textwidth]{Turbofan_operation}
	\caption{Illustration of a Turbofan engine (CC SA-BY  \href{http://en.wikipedia.org/wiki/File:Turbofan_operation.svg}{K. Aainsqatsi})}
	\label{turbofan_illu}
\end{figure}

To illustrate how an optimization problem is modeled, we use a simplified Turbofan optimization problem. On \figurename{} \ref{turbofan_illu}, an illustration of the principle of the turbofan can be seen. In this figure, the bypass ratio is the ratio between the air drawn in by the fan not entering engine core (which is \emph{bypassed}) and the air effectively used for the combustion process. The pressure ratio is the ratio between pressure produced by the compressors and the pressure it receives from the environment.

In order to identify the elements of a generic continuous optimization model, we worked with experts from several related fields: numerical optimization, mechanics as well as aeronautics and engine engineers. As a result, we identified five classes of interacting entities: \emph{models}, \emph{design variables}, \emph{output variables}, \emph{constraints} and \emph{objectives}. These entities and their relations are represented by the diagram in \figurename{} \ref{class_diag}, that we detail next.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{class_diag}
	\caption{Class diagram of MDO problems}
	\label{class_diag}
\end{figure}

In \figurename{} \ref{turbofan:math}, the analytic expression of this optimization problem is given, while in \figurename{} \ref{turbofan:graph}, the problem is presented as a graph of the different entities. The design variables of this problem are $pi\_c$ and $bpr$, which indicate respectively the compressor pressure ratio and the bypass ratio of the engine. The turbofan model produces three outputs: $Tdm0$, $s$ and $fr$, representing respectively the thrust, fuel consumption and thrust ratio of the engine. In this problem we try to maximize the thrust and minimizing the fuel consumption while satisfying some feasibility constraints. 

\begin{figure}[]
\centering
	\begin{subfigure}[b]{0.4\textwidth}
		$\begin{array}{c}
			(Tdm0, s, fr) = Turbofan(pi\_c, bpr) \\
			max \; Tdm0 \\
			min \; s \\
			subject \; to \\
			s \leq 155 \\
			fr \geq 4
		\end{array}$
		\caption{mathematical formulation.}\label{turbofan:math}
	\end{subfigure}
	\hfill%
	\begin{subfigure}[b]{0.55\textwidth}
			\centering
			\includegraphics[width=\textwidth]{testcases-Turbofan-annoted}
			\caption{corresponding entities graph.}\label{turbofan:graph}
	\end{subfigure}

\caption{Turbofan problem.}
\label{turbofan}

\end{figure}

Let's now see in more details the roles of each of these fives entities: \emph{model}, \emph{variable}, \emph{output}, \emph{constraint} and \emph{objective}.

\subsubsection*{Models.}

In the most general case, a \emph{model} can be seen as a black box which takes input values (which can be \emph{design variables} or \emph{output variables}) and produces output values. A \emph{model} represents a technical knowledge of the relations between different parts of a problem and can be as simple as a linear function or a much more complex algorithm requiring several hours of calculation. Often some properties are known (or can be deduced) about a model and specialized optimization techniques can exploit this information. It is important to understand that what is exactly a \enquote{model} is quite an arbitrary choice, based on the application domain of the problem. Depending on the goals and needs, a same problem will be divided in models of different granularity and scope. A model can represent a simple calculus step as well as an entire discipline.
In our Turbofan example, a \emph{model} entity is the $Turbofan$ function which calculate the three outputs using the values of $bpr$ and $pi\_c$.

\subsubsection*{Design Variables.}

These are the inputs of the problem and can be adjusted freely (within their defining boundaries). The goal is to find the set(s) of values for these variables that maximize the objectives while satisfying the constraints.
\emph{Design variables} are used by \emph{models} to calculate their outputs and by constraints and objectives to calculate their current value. A \emph{design variable} can be shared among several \emph{models}, objectives and constraints.
Keeping with our example, $bpr$ and $pi\_c$ are the two \emph{design variables} of our optimization problem.

\subsubsection*{Output Variables.}

These values are produced by a \emph{model}, and consequently cannot be changed freely.
As for the \emph{design variables}, the \emph{output variables} are used by \emph{models} to calculate their outputs and by constraints and objectives to calculate their current value.
In our example, $Tdm0$, $s$ and $fr$ are \emph{output variables} produced by the $Turbofan$ model.

\subsubsection*{Constraints.}

These are strict restrictions on some parts of the problem, represented as functional constraints defined by equalities and/or inequalities. These can be the expression of a physical constraint, or a requirement concerning the problem.
Regarding the Turbofan, the two \emph{constraints} are $s <= 155$ and $fr >=4$.

\subsubsection*{Objectives.}

The goals to be optimized. In the general case, different objectives are often contradictory.
The two \emph{objectives} of the Turbofan problems are to maximize $Tdm0$ and to minimize $s$.

Constraints and objectives are usually regrouped under the more general term of optimization criteria. 

\paragraph*{}
An interesting and important point is that both models, constraints and objectives involve computation. Often the most heavyweight calculus is encapsulated inside a model and the calculi concerning criteria tend to be simple equations, but this is neither an absolute requirement nor a discriminating characteristic.

Using this decomposition, the optimization problem can then be represented as a graph $ G = (V, E)$, where the vertices set $V$ is a tuple $\langle \mathcal{V}_d, \mathcal{V}_o, \mathcal{M}, \mathcal{O}, \mathcal{C}\rangle$ (denoting respectively the sets of design variables, output variables, models, objectives and constraints of the problem), and $E$ is the set of relations between the entities. It can be noted that the graph is bipartite regarding the two sets $(\mathcal{V}_d \cup \mathcal{V}_o)$ and $(\mathcal{M} \cup \mathcal{O} \cup \mathcal{C})$, as design and output variables can only be connected to models, objectives or constraints, which can themselves only be connected to design or output variables.
Using our Turbofan example, the nodes tuple $\langle \mathcal{V}_d, \mathcal{V}_o, \mathcal{M}, \mathcal{O}, \mathcal{C}\rangle$ of the graph representing this problem is defined as:
\begin{align*}
	\mathcal{V}_d &= \{bpr, pi_c\}, \\
	\mathcal{V}_o &= \{Tdm0, s, fr\},\\
	\mathcal{M} &= \{Turbofan\_Model\},\\
	\mathcal{O} &= \{\text{max }Tdm0, \text{min }s\},\\
	\mathcal{C} &= \{s <= 155, fr >= 4\}.
\end{align*}

The NDMO modeling aims to provide the most complete and natural representation of the problem. This modeling preserves the relations between the domain entities and is completely independent of the solving process.

\section{From an Optimization Problem to a Multi-Agent System}

Based on the NDMO modeling in section \ref{modeling}, we propose a multi-agent system where each domain entity is associated with an agent. Thus the multi-agent system is the representation of the problem to be solved with the links and communication between agents reflecting the natural structure of the problem. It is worth underlining the fact that this transformation (\textit{i.e.} the agentification) can be completely automatic as it is fully derived from the expression of the problem.

\begin{figure}
\includegraphics[width=\textwidth]{agent_class_diag}
\caption{System class diagram}\label{SMA_class_diagram}

\end{figure}

The solving process - constituted by the collective behavior of the agents - basically relies, on change-value requests sent by the criteria agents resulting in cooperatively decided adjustments done by the \emph{design variables} and on new values computed by the models resulting on the satisfaction or dissatisfaction of the criteria agents. 
In the same way we presented the different elements of NDMO, we now detail the behaviors of our five agent types: \emph{model}, \emph{variable}, \emph{output}, \emph{constraint} and \emph{objective} agents.
A summary of the basic principles of each agent type is given in Algorithm \ref{agent_algo}.

\subsubsection*{Model Agent.}

A \emph{model agent} takes charge of a model of the problem. It interacts with the agents handling its inputs (which can be \emph{variable} or \emph{output agents}) and the \emph{output agents} handling its outputs. Its individual goal is to maintain the consistency between its inputs and its outputs. To this end, when it receives a message from one of its inputs informing it of a value change, a \emph{model agent} recalculates the outputs values of its model and informs its\emph{output agents} of their new value. On the other part, when a \emph{model agent} receives a message from one of its \emph{output agents} it translates and transmits the request to its inputs. 

To find the input values corresponding to a specific desired output value, the \emph{model agent} uses an external optimizer. This optimizer is provided by the engineer based on expert domain-dependent knowledge regarding the structure of the model itself.
It is important to underline that the optimizer is used only to solve the local problem of the \emph{model agent}, and is  not used to solve the problem globally.


%\begin{algorithm}
%\caption{Behavior of a Model Agent}
%\begin{algorithmic}
%\While{running}
%	\State Receive Messages
%	\If{received new informs}
%        \State recalculate outputs
%        \State inform output agents
%    \EndIf      
%	\If{received new requests}
%       \State  use optimizer to find adequate inputs
%        \State propagate requests to input agents
%    \EndIf
%\EndWhile
%\end{algorithmic}
%\end{algorithm}

\subsubsection*{Variable Agent.}

This agent represents a \emph{design variable} of the problem. Its individual goal is to find a value which is the best equilibrium among all the requests it can receive (from models and criteria for which it is an input). The agents using the variable as input can send to it request asking to change its value. When changing value, the agent informs all agents linked to it of its new value. 

%\begin{algorithm}
%\caption{Behavior of a Variable Agent}
%\begin{algorithmic}
%\While{running}
%	\State Receive Messages  
%	\If{received new requests}
%       \State select most important
%       \State adjust value
%       \State inform related agents
%    \EndIf
%\EndWhile
%\end{algorithmic}
%\end{algorithm}

\subsubsection*{Output Agent.}
The \emph{output agent} takes charge of an output of a model. \emph{Output agent} and \emph{variable agents} have similar roles, except \emph{output agents} cannot directly change their value. Instead they send a request to the \emph{model agent} they depend on. In this regard, the \emph{output agent} act as a filter for the \emph{model agent} it depends on, selecting among the different requests the ones it then transmits.

%\begin{algorithm}
%\caption{Behavior of an Output Agent}
%\begin{algorithmic}
%\While{running}
%	\State Receive Messages
%	\If{received new informs}
%        \State update its value
%        \State inform related agents
%    \EndIf      
%	\If{received new requests}
%       \State  select most important
%        \State transmit selected request to model agent
%    \EndIf
%\EndWhile
%\end{algorithmic}
%\end{algorithm}

As we will see in the next section, the \emph{output agent} is distinct from the \emph{variable agent} in the way that it can be involved in cycles. A cycle is a situation of interdependent models (that is, models which depend of each other to calculate their outputs).

%We describe in the next section the main difficulties when solving an optimization problem including interdependencies, and how can an \emph{output agent} detect and handle them.

\subsubsection*{Constraint Agent.}
 \emph{The constraint agent} has the responsibility for handling a constraint of the problem. When receiving a message from one of its inputs, the agent recalculates its constraint and checks its satisfaction. If the constraint is not satisfied, the agent sends \emph{change value} requests to its inputs.

%\begin{algorithm}
%\caption{Behavior of a Constraint/Objective Agent}
%\begin{algorithmic}
%\While{running}
%	\State Receive Messages
%	\If{received new informs}
%        \State update its value
%        \State use optimizer to find adequate inputs
%        \State send new requests to input agents
%    \EndIf      
%\EndWhile
%\end{algorithmic}
%\end{algorithm}

It should be noted that, to estimate the input values required to satisfy the constraint on its computed value, this agent employs the same technique as the \emph{model agent} (\textit{i.e.} an external optimizer).

\subsubsection*{Objective Agent.}
The  \emph{objective agent} is in charge of an objective of the problem. This agent sends requests to its inputs aiming to improve its objective, and recalculates the objective when receiving  \emph{value changed} messages from its inputs.

This agent uses an external optimizer to estimate input values which would improve the objective, as the model and constraint agents.


\chapter{Agents Behavior}

We will now discuss the behavior the the five agent types identified in the previous chapter. The functioning of the system can be divided into two main tasks: problem simulation and collective solving.

Problem simulation can be seen as the equivalent of the analysis of classical MDO method. The agents behavioral rules related to problem simulation concern the propagation of the values of design variables to the models and criteria based on the value. For this part, the agents will exchange \emph{inform} messages which contains calculated values. The \enquote{message flow} is top-down: the initial inform messages will be emitted by the variable agents and will be propagated down to the criteria agents. The illustration of the messages flow of simulation is shown on \figurename{} \ref{messages_flow:inf}.

Collective solving concerns the optimization of the problem. The agent behavioral rules related to collective solving are about satisfying the constraints while improving the objectives. For this part, the agents will exchange \emph{request} messages which contains desired variations of values. The \enquote{message flow} is bottom-up: the initial request messages will be emitted by the criteria agents and propagated up to variable agents. The illustration of the messages flow of solving is shown on \figurename{} \ref{messages_flow:req}.

\begin{figure}[h]
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{testcases-Turbofan-informs}
		\caption{Informs flow for simulation}\label{messages_flow:inf}
	\end{subfigure}
	\hfill
  	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{testcases-Turbofan-requests}
		\caption{Requests flow for solving}\label{messages_flow:req}
	\end{subfigure}

	%new line
	\centering
	 \begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{testcases-Turbofan-informs_requests}
		\caption{Effective messages flow}\label{messages_flow:eff}
	\end{subfigure}
	
	\caption{Messages flow for simulation and solving}
	\label{messages_flow}

\end{figure}

The agents behaviors regarding these two parts can be studied quite independently, we will thus present them separately.  It is however important to remember that these two parts are not executed separately. At runtime the agents will simulate the problem and solve it in parallel. Moreover, the different parts of the system will not necessarily work in a synchronous fashion. The effective messages flow of the system will more probably be akin to the \figurename{} \ref{messages_flow:eff}.

\section{Problem Simulation}

In this section we will present the agents behavior related to the simulation of the problem. Regarding this part, the main concern of the agents is to ensure consistency between the values of the design variables and the produced output. To this end, the agents will propagate \emph{Inform} messages through the system.

An inform message carry a new value $v$. The exact semantic of this information slightly changes depending on which agents are involved:

\begin{itemize}
\item If the message is sent from a value agent (variable or output) to a model or criterion agent, it indicates to the receiving agent that the sending agent has changed value.

\item If the message is sent from a model agent to an output agent, it indicates to the receiving agent that the model has calculated its new value.
\end{itemize}

In practice this distinction is not fundamentally important to understand the functioning of the system.

As stated in section \ref{modeling}, models, constraints and objectives involve a specific calculation operation. For example the constraint $x -y \geq 0$ implies the very basic calculation $x - y$. We regroup these operations under the term \emph{internal models}. As we said, no specific hypothesis is done concerning the nature of an internal model and, more importantly, no distinction is done regarding the internal model used by a model agent and the one used by an objective or a constraint agent.\\
While in some cases specific informations can be known regarding the nature of an internal model, in the most general case these internal models can be seen as black boxes and are handled as such by the agents.

\subsection{Variable Agent}

Regarding problem simulation, the role of the variable agent is to ensure that the agents to which it is connected know its current value.
A variable agent has to send new Inform messages when:
\begin{compactitem}
\item new agents are connected to it (typically at the creation of the system)
\item it decided to change its value based on received requests (see \ref{variable_agent_solving})
\item the designer changed its value
\end{compactitem}

The behavior of the variable agent is summarized in algorithm \ref{algo_simulating_variable}.

\begin{algorithm}
\caption{Problem Simulation - Variable Agent Behavior}
\label{algo_simulating_variable}

	$C \leftarrow$ previously connected agents\;
	$C'\leftarrow$  newly connected agents\;
		
	$v \leftarrow$ previous value\;
	$v'\leftarrow$ new value\;
	
	\tcp{notify contacts of value change}
	\If{$v \neq v'$}{
		$v \leftarrow v'$\;
		\ForEach{agent $a \in C$}{
			send($a$, new Inform($v$))\;
		}
	}
	
	\tcp{send its value to new contacts}
	\If{$C' \neq \emptyset$}{
		\ForEach{agent $a \in C'$}{
			send($a$, new Inform($v$))\;
		}
		$C \leftarrow C + C'$\tcp{memorize new connected agents}
	}
	
\end{algorithm}

\subsection{Model Agent}\label{simulation_model}

A model agent must maintain consistency between its inputs and outputs. When receiving an Inform message from one of the agents controlling its inputs, the model agent reevaluates its internal model taking the new value into account. Using the new values produced by the internal model, the model agents then send Informs messages to the agents responsible of its outputs.

All outputs of a model agent are not necessary associated with an ouput agent. For example, if the designer is not interested by the value represented by one of the outputs, he is not required to represent it by an agent. In this case the model agent will silently calculate the new value of the output at the same time than the others, but will not propagate it\footnote{Remember that the internal model of the agent is a black box, consequently the agent cannot choose to evaluate it partially. If the evaluation of the output could has been done independently from the others, it could be preferable to create two separate models}.

However, as the problem is dynamic, the designer can decide at any time to connect a new output agent to a previously unconnected output. In this case, the model agent must send to the output agent the last value it calculated for this output.

The behavior of the model agent is summarized in algorithm \ref{algo_simulating_model}.

\begin{algorithm}
\caption{Problem Simulation - Model Agent Behavior}
\label{algo_simulating_model}

	$C \leftarrow$ previously connected output agents\;
	$C'\leftarrow$  newly connected output agents\;
	
	$\{v_i\} \leftarrow$ new input values\;
			
	\If{$\{v_i\} \neq \emptyset$}{
		$\{v_o\} \leftarrow$ previous output values\;
		$\{v'_o\}\leftarrow Internal\_Model(\{v_i\})$\tcp{use internal model to recalculate outputs}
		\ForEach{agent $o \in C$}{
			\tcp{inform output agents of their new value}
			send($o$, new Inform($v'_o$))\;
		}
	}
	
	\tcp{Send their value to new outputs}
	\If{$C' \neq \emptyset$}{
		\ForEach{agent $o \in C'$}{
			send($o$, new Inform($v'_o$))\;
		}
		$C \leftarrow C + C'$\tcp{memorize new outputs agents}
	}
	
\end{algorithm}

\subsection{Output Agent}

For the problem simulation, the output agent has a very similar role to the variable agent. It too tries to ensure that the agents to which it is connected know its current value. It has to change new Inform messages when:
\begin{compactitem}
\item new agents are connected to it
\item it receives an Inform message from its model agent, indicating it its new value
\end{compactitem}

Unlike the variable agent, the output agent value should not be directly changed by the designer, as it represents a non-free variable.

The behavior of the output agent is summarized in algorithm \ref{algo_simulating_output}.

\begin{algorithm}
\caption{Problem Simulation - Output Agent Behavior}
\label{algo_simulating_output}

	$C \leftarrow$ previously connected agents\;
	$C'\leftarrow$  newly connected agents\;
		
	$v \leftarrow$ previous value\;
	$v'\leftarrow$ new value\;
	
	\tcp{notify contacts of value change}
	\If{$v \neq v'$}{
		$v \leftarrow v'$\;
		\ForEach{agent $a \in C$}{
			send($a$, new Inform($v$))\;
		}
	}
	
	\tcp{send its value to new contacts}
	\If{$C' \neq \emptyset$}{
		\ForEach{agent $a \in C'$}{
			send($a$, new Inform($v$))\;
		}
		$C \leftarrow C + C'$\tcp{memorize new connected agents}
	}
	
\end{algorithm}

At runtime the designer can interact with the MAS. Among the possible interactions, the designer can remove the model calculating an output. In this case the output agent would become a variable agent and change its behavior accordingly. The inverse transformation is also possible, a variable agent could suddenly be \enquote{plugged} in output of a model and would become an output agent.

\subsection{Constraint/Objective Agent}

In regard to problem simulation, constraint and objectives agents sole role is to update their value based on the Inform they receive from their inputs.

Their behavior is summarized in algorithm \ref{algo_simulating_criterion}.

\begin{algorithm}
\caption{Problem Simulation - Constraint/objective Agent Behavior}
\label{algo_simulating_criterion}

	$\{v_i\} \leftarrow$ new input values\;
			
	\If{$\{v_i\} \neq \emptyset$}{
		$\{v'_o\}\leftarrow Internal\_Model(\{v_i\})$\tcp{use internal model to recalculate outputs}
	}

\end{algorithm}

\section{Collective Solving}\label{collective_solv}

During solving, the criteria agents try to improve their local goal. That is, the constraints agents try to keep their constraint satisfied, while the objective agents try to improve their objective. To this end they send \emph{Request} messages to the agents controlling their inputs, asking them to change value. The others agents have to propagate these requests toward the variable agents in the most adequate way.

However a specific difficulty arises. As explained in the previous section, model, constraint and objective agents manipulate the underlying equations (or algorithms, or responses surfaces \emph{etc.}) of the problem through internal models, which are in most cases black boxes. In this case, how can constraint agents know which values to ask to satisfy their constraint, how can objective agents know which values improve their objective, how can model agents know which values for their outputs could produce the values required by their outputs?\\
For the internal models agents to be able to work, the designer must supply them with an \emph{external optimizer}. The role of this optimizer is very simple: if we see an internal model as a function\footnote{Calling an internal model a function is a simplification. Since it is a black box it may have an internal state. Consequently the same inputs may not always produce the same outputs} providing an output $O$ from a set of input $I$, what we need is to be able to estimate the inverse function, providing $I$ corresponding to a specific $O$. This specific problem can easily be expressed as a classical optimization problem in itself, which can be solved by a classical optimization method. As the agents manipulate black boxes, it is (nearly\footnote{We will see in section \ref{model_agent_solving} how can the agents can extract enough information at runtime to provide their own basic optimizer}) impossible for them to do this estimation by themselves. The designer, which have a more advanced knowledge of the internal models, can provide an adequate optimization method for each of them. In the most general case it can be assumed that each agent in control of an internal model uses its own specific optimizer. However in practice several instances of the same optimizer can be used by different agents, if the designer deems that their models present sufficient similar characteristic for the optimization method to be efficient for each of them. An example of use of an external optimizer by a model agent can be seen on \figurename{} \ref{optim_use}.

In order to keep the presentation of the agents behavior simple, we will assume for the moment that no agent ever receive simultaneous contradictory requests. We will see in section \ref{NCS_pres} the advanced selection mechanisms used by the agents to handle such cases.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[height=5cm]{optim_use_1}
		\caption{The model agent receives a request.}\label{optim_use:1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[height=5cm]{optim_use_2}
		\caption{The model agent resolves the request using an external optimizer.}\label{optim_use:2}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[height=5cm]{optim_use_3}
		\caption{The model agent transmits the new requests to its inputs.}\label{optim_use:3}
	\end{subfigure}
	\caption{Use of an external optimizer.}\label{optim_use}
\end{figure}

With this specificity in mind, let's now examine the behavior of the different agent types in the context of collective solving.

\subsection{Variable Agent}\label{variable_agent_solving}

During solving, a variable agent is susceptible to receive change requests from others agents. When a value agent receives a change request, it tries to change its own value  in order to accommodate the requester while trying taking in account the previous demands of the rest of its neighbors. To this end, the variable agent  uses an exploration strategy based on \emph{Adaptive Value Tracker} (AVT) \cite{Lemouzy_2011}. The AVT can be seen as an adaptation of dichotomous search for dynamic values. The main idea is to change value according to the direction which is requested and the direction of the past requests. While the value varies in the same direction, the variation delta is increased so the value varies more and more. As soon as the requested variation changes, it means that the variable went past the good value, so the variation delta is reduced.
This capability to take into account a changing solution allows the variable agent to continuously search for an unknown dynamic target value. This capability is required for the system to be able to adapt to changes made by the engineer during the solving process.

While changing value based not on the value requested but on the direction can seem paradoxical, it must be recalled that, since no agent has a global view of the system, the requests made by the agents is often approximate, so the agents need to iterate many times. If the search space in large, the system could take time to converge towards the solution. By using a near-dichotomous strategy, we greatly accelerate this convergence.

[[Give analytical formulation of the AVT]]

One could wonder why the variable agent uses such a seemingly contrived method instead of just taking the requested value. It should be remembered that the variable agent can be connected to several parts of the problem and can receive contradictory requests from different origins over time (as we said, the case where the variable receives contradictory requests at the same time will also be studied later). Thus, the variable agent must have a way to take in account these contradictory requirements to find an adequate compromise. Moreover, until the system (or at least this part of it) has stabilized, the values requested has very little chance of being exact. The value can thus be considered as a  indication of the direction in which the requesting agent desire the variable to go. As the system stabilizes, the precision of the AVT is improved and the value of the variable is closer and closer to the exact correct value.

This behavior is summarized in algorithm \ref{algo_solving_variable}.

\begin{algorithm}
\caption{Collective Solving - Value Agent Behavior}
\label{algo_solving_variable}

	$v \leftarrow$ old value\;
	$v_r \leftarrow$ requested value\;
	$avt \leftarrow$ agent AVT \;
	
	\eIf{$v < v_r$}{
		$feedback \leftarrow$ INCREASE\;
	}{
		$feedback \leftarrow$ DECREASE\;
	}
	$v' \leftarrow$ $avt$.adjustValue($feedback$)\;
	
\end{algorithm}

\subsection{Model Agent}\label{model_agent_solving}

The model agent is responsible for transmitting requests it received from its outputs to its inputs. To be able to translate the ingoing requests to its inputs, the model agent uses an external optimizer, as presented in introduction. When receiving a request from one of its outputs, the model agent calls the external optimizer, which provides it with the adequate inputs values corresponding to the request. The model agent then sends requests corresponding to these values to its inputs.
While the model agent can seem to be very simplistic, we will see in section \ref{collective_solv} how its 
behavior can be expanded to handle multiple problematic cases.

The behavior of the model agent is summarized in algorithm \ref{algo_solving_model}.

\begin{algorithm}
\caption{Collective Solving - Model Agent Behavior}
\label{algo_solving_model}

	$v_r \leftarrow$ requested value from output\;
	$M \leftarrow$ the internal model of the agent\;
	$O \leftarrow$ associated optimizer\;
	
	$\{v_r^i\} \leftarrow $O($v_r$, $M$) \tcp{get the input values estimated by the optimizer}\;
	\ForAll{$i \in$ input controlling agents}{
		send($i$, new Request($v_r^i$))\;
	}	
\end{algorithm}

[[Explain here ? if not, need to changes refs to label model_agent_solving]]

At the start of the section we stated that, since the internal model is a black box, the model agent cannot in itself perform the \enquote{bottom-up} translation from the values requested by its outputs to values to ask to its inputs, requiring an external optimizer to carry such task. While this is true at the start of the process, the model agent can at runtime observe the values returned by the model and make some estimations about its internal topology. We present now such a mechanism which can be integrated into model agents, capable of taking advantage of the functioning of the agent using simple black-box optimization techniques.

This mechanism integrates itself during the simulation behavior of the agent. Remember that, as presented in section \ref{simulation_model}, when receiving a new value from one of its input the model agent calls its internal model to recalculate the values of its outputs. At this step, instead of passing all the the agent can observe the impact of the variation of variation of the input on each output. We call \emph{correlation} between the input $i$ and output $o$ the variation of $o$ relative to the variation of $i$, calculated as $\dfrac{v_o^{t+1} - v_o^t}{v_i^{t+1} - v_i^t}$. This correlation can be seen as a local linear approximation of the partial derivative $\dfrac{\delta o}{\delta i}$.This correlation can then be used during solving to estimate the required changes from the inputs to satisfy a change requested by an output.\\
For example, suppose a model with one input $i$ and one output $o$. During simulating, the value of $i$ changes from 1 to 2. As a consequence the internal model provides a change of $o$ from -2 to -4. Thus the correlation between $i$ and $o$ is $\dfrac{-4 - (-2)}{2 - 1} = -2$. The output $o$ then sends a request to the model agent to take the value 4. The agent can estimate that for $o$ to take the value 4, $i$ needs to take the value $\dfrac{4}{-2} = -2$.
The complete formula when several inputs are involved in the calculus of $o$ is presented with the summary of the mechanism in algorithm \ref{algo_solving_internaloptim}.

\begin{algorithm}
\caption{Collective Solving - Internal Optimizer Algorithm}
\label{algo_solving_internaloptim}

\emph{estimating correlation deltas}\;
$I \leftarrow$ current input values\;
$O \leftarrow$ current output values\;
	\For{$inform \leftarrow $ \{received inform messages\}}{
		$i_{t-1} \leftarrow $ old value of $i_{inform}$\;
		$i_{t} \leftarrow $ new value of $i_{inform}$\;
		
		$\{o_{t-1}\} \leftarrow \{$ old value of $o, \forall o \in O\}$\;
		\BlankLine
		\tcp{Reevaluate the outputs using the new input value}
		$I_{t} \leftarrow \{i \in I, i \neq i_{t-1}\} \cup \{i_t\}$\;
		$\{o_{t}\} \leftarrow M(	I_{t})$\;
		\BlankLine
		\tcp{For each output, calculate the new correlation $\delta$}
		\For{$o \leftarrow O$}{
			$\delta_{i_{inform}, o} \leftarrow 	\dfrac{o_{t} - o_{t-1}}{i_{t} - i_{t-1}}$\;
		}
	}

\BlankLine\BlankLine
\emph{linear approximation optimization}\;
$o^* \leftarrow $ target value for $o$\;
$\Delta_o \leftarrow o^* - o_{current}$\;
$\{\delta_i\} \leftarrow $ estimated correlations between all the inputs and the output\;
\For{$i \leftarrow $ inputs}{
	\tcp{calculate the input variation}
	$\Delta_i \leftarrow \Delta_o \times  \dfrac{\delta_i}{\displaystyle\sum_{i \in \text{inputs}}{\delta_i}}$\;
}
\end{algorithm}

We presented here a very simple learning mechanism for the agent to default when it is not provided with an external optimizer. This mechanism is voluntary simple and lightweight, in order to be applicable to a broad range of model topologies. While it is possible to make further refinements to improve its results, we believe that such needs are better covered by the use of external optimizers, which can be tailored for specific needs.\\
Interestingly, our approximation technique can also be used to create local linear approximations of complex black box models. We will see in the later sections how such information can be used by the system to solve some corner cases.

\subsection{Output Agent}

Like for simulation, output agents have a similar role to variable agents during problem solving. However, as it depends of a model agent for its value, an output agent will simply transmits the requests it receives to its model, as summarized in algorithm \ref{algo_solving_output}.

\begin{algorithm}
\caption{Collective Solving - Output Agent Behavior}
\label{algo_solving_output}

	$v_r \leftarrow$ requested value\;
	$m \leftarrow$ the model agent responsible of the output \;
	
	send($m$, $v_r$) \tcp{forward the request to the model agent}\;
	
\end{algorithm}

\subsection{Constraint Agent}

With the objective agents, the constraint agents are the origins of the requests which are propagated into the system. The goal of a constraint agent is to ensure the constraint of which it is in charge is satisfied. To this end, the constraint agent calculates a target value for the constraint, estimates corresponding target values for its inputs and sends them as requests to the agents in charge of these inputs. When the constraint is satisfied, the constraint agent tries to ensure it will keep being satisfied. To this end it continues to send requests to move its current value further and further from the threshold.

The problematic behind finding corresponding input values for the target value is similar to the one of the model agent of finding adequate inputs values from the outputs. As we said at the end of section \ref{modeling}, constraint agents (and objective agents) are similar to model agents in the fact that both have control of an \emph{internal model}. Thus, constraint agents can, in the same way than model agents, make use of an external optimizer to find adequate target values for its inputs. The only difference being that, instead of having a value requested by an output, the constraint agent estimates itself its own target value.\\
In the same way, constraint agents can use the same internal optimization mechanism than model agents, presented in algorithm \ref{algo_solving_internaloptim},  when not provided with an external optimizer.

The behavior of a constraint agent with a \enquote{lower or equal} constraint is described in algorithm \ref{algo_solving_constraint}. This algorithm is easily adapted to the others constraint types.

\begin{algorithm}
\caption{Collective Solving - Constraint Agent Behavior}
\label{algo_solving_constraint}

$v \leftarrow $ current value\;
$v_t \leftarrow $ threshold value\;
$\Delta^* \leftarrow |v - v_t|$\;

\BlankLine
$\{ \Delta^*_i \} \leftarrow Optimizer(v - \Delta^*)$\;
\tcp{can be either external optimizer or internal optimization mechanism}
\For{$i \leftarrow inputs$}{
	\tcp{change the estimated target value to the input}
	send($i$, $\Delta^*_i$)\;
}
\end{algorithm}

It should be noted that, even when the constraint is satisfied, the constraint agent continues to send requests to its inputs. This behavior can be justified by several reasons.  First of all, the goal of the constraint agent is to ensure its constraint stays satisfied, so when the constraint is already  satisfied the constraint agent has interest in \enquote{pushing} its current value afar from the frontier.\\
The other reason is to help its neighbors agents keep a coherent view of their relations. Indeed, as the system is interactive, the designer can decide at any time to remove or change the constraint. As a consequence it is not possible for the agents to simply keep in memory the constraints they are linked to, as these informations can become obsolete at any time. A simple way to remedy to this situation is for the constraint to not assume the others agents will memorize its requirements and to keep sending them requests.

\subsection{Objective Agent}

Along with constraint agents, the objective agents are the origins of the requests which are propagated into the system. The goal of an objective agent is to improve the objective of which it is in charge. To this end, the objective agent iteratively estimates a new target value for its objective, finds the corresponding input target values and sends them as requests to the input agents. The behavior of the objective agent in this regard is quite similar to the one of the constraint agent. The difference is that, while the constraint agent has a reference value to target (the threshold value), the objective agent has no such value. Instead, the objective agent will target a new value based on the last variation of its value.

Like the constraint and model agents, the objective agent can use an external optimizer to find the corresponding intput values, or can use the internal optimization mechanism presented in algorithm \ref{algo_solving_internaloptim}.

The behavior of an objective agents with a \enquote{minimize} objective is described in algorithm \ref{algo_solving_objective}. This algorithm is easily adapted to maximization objectives.

\begin{algorithm}
\caption{Collective Solving - Constraint Agent Behavior}
\label{algo_solving_objective}

$v \leftarrow $ current value\;
$\Delta_{t-1} \leftarrow $ last variation of value \;

\BlankLine
$\{ \Delta^*_i \} \leftarrow Optimizer(v - Delta_{t-1})$\;
\tcp{can be either external optimizer or internal optimization mechanism}
\For{$i \leftarrow inputs$}{
	\tcp{change the estimated target value to the input}
	send($i$, $\Delta^*_i$)\;
}
	
\end{algorithm}

It is interesting to note that the objective agent exhibits a behavior similar to the one of the constraint agent, but for slightly different reasons. While the constraint agent continuously sends requests in order to make its constraint safer and safer, the objective agent continuously send requests because it can never knows if it reached the best value for the objective it handles. An objective agent is essentially \enquote{blind}, as the objective is a black box, and must rely on the external optimizer (or the internal optimization mechanism) in order to improve it. Consequently, the objective agent never stops trying to find a better value. As with constraint agents, this mechanism is quite handy in the context of interactive optimization, as the designer can at any moment completely change the nature of the objective.

\section*{}
An important point is that each agent only has a local strategy. No agent is in charge of the optimization of the system as a whole, or even of a subset of the other agents. Contrary to the classical MDO methods presented earlier, the solving of the problem is not directed by a predefined methodology, but by the structure of the problem itself. The emerging global strategy is unique and adapted to the problem.

\begin{algorithm}
\caption{Agents Behaviors}\label{agent_algo}

\SetKwProg{Bv}{behavior of }{}{}

\Bv{Model Agent}{
	\Repeat{resolution end}{
		analyze received messages\;
		\If{received new information messages}{
	        recalculate outputs\;
	        inform depending agents\;
	    }     
		\If{received new requests}{
	        use optimizer to find adequate inputs\;
	        propagate requests to input agents\;
	   }
	}
}

\Bv{Variable Agent}{
	\Repeat{resolution end}{
		analyze received messages\;
		\If{received new requests}{
	       select most important\;
	       adjust value\;
	       inform depending agents\;
	    }
	}
}

\Bv{Output Agent}{
	\Repeat{resolution end}{
		analyze received messages\;
		\If{received new information messages}{
	        update its value\;
	        inform depending agents\;
	    }      
		\If{received new requests}{
	        select most important\;
	        transmit selected request to model agent\;
	    }
    }
}

\Bv{Constraint/Objective Agent}{
	\Repeat{resolution end}{
		analyze received messages\;
		\If{received new information messages}{
	        update its value\;
	        use optimizer to find adequate inputs\;
	        send new requests to variable/output agents\;
	    }
	}
}

\end{algorithm}


\section{Non-Cooperative Situations}\label{NCS_pres}

In the previous sections, we presented the basic agent behavior of our system. Algorithm \ref{agent_algo} is a synthesis of the behavior of each agent type. While this basic behavior could suffice in very simple test cases, it is not sufficient to handle the specificities of most continuous optimization problem configurations. In these situations, the nominal agent behavior (presented in the previous section) would lead to a suboptimal result.\\
Basing ourselves on the AMAS theory (presented in section \ref{amas_theory}), we can consider these configurations to be Non Cooperative Situations (NCS), as they represent a situation where, because of a shortcoming in the behavior of the agents, the system does not produces an adequate functionality (in our case, the optimization of the problem).\\
Consequently, we need for each NCS to provide the agents with specific mechanisms to:
\begin{compactenum}
\item detect occurring instances of the NCS
\item solve detected instances of the NCS
\item if possible, anticipate future instances of the NCS and avoid them
\end{compactenum}

Methodologically, by studying how the system handles specific problems with characteristics which are specific to continuous optimization (interdependencies, conflicting criteria \emph{etc.}), we identified several problematic configuration types, and defined different cooperation mechanisms for the agents that enable the system to correctly solve problems which exhibit these characteristics.

We will now present some additional mechanisms to handle some specific problematic cases we identified.

\subsection{Conflicting Requests}

[[mettre les refs vers les travaux existants sur la criticité plutot dans la partie sur les CPSP ?]]

The first problematic configuration, and arguably the most obvious, is the one where an agent is in position of receiving simultaneous conflicting requests. This situation concerns every agent which is susceptible to receive requests: variable, model and output agents. This situation can be qualified as a \emph{Conflict} NCS, as the requesting agents ask for contradictory modifications of their environment. A minimal example of configuration in which conflicting requests may appear is shown in \figurename{} \ref{NCS_conflicting_requests}.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{NCS_conflicting_requests}
\caption{Conflicting trajectories example}\label{NCS_conflicting_requests}
\end{figure}

When such an agent receives contradictory requests, it needs a way to select which request to apply and which request to reject. We can intuitively see how this decision should be based on the current state of the requesting agents. For example, when receiving requests both a satisfied and an non-satisfied constraints, a variable agent should favor the non-satisfied one. In order for the agents to be able to make this decision, we need to provide them with a way to compare the states of different agents.

To this end, we introduce a new mechanism based on a specific measure called \emph{criticality}. This measure represents the state of dissatisfaction of the agent regarding its local goal. Each agent is in charge of estimating its own criticality and providing it to the other agents\footnote{We do not concern ourselves with the problematic of trust here, each agent is assumed to provide the most trustful and accurate information without cheating or lying. The use of measures such as criticality in open and untrusted environments is in itself an interesting question to say the least.}. The role of this measure is to aggregate into a single comparable value all the relevant indicators regarding the state of the agent. Having a single indicator of the state of the agent is interesting as it simplifies the reasoning of the agents. In addition, this mechanism has the interesting property of limiting the informations transmitted to the others agents, which can be of interest in case of a distributed optimization where data privacy is an issue.
However the system designer has the difficult task to provide the agents which adequate means to calculate their criticality. Also, in specific cases, this information on the state of the agents is not sufficient to take the correct decision, as we will see in the following sections.

%%
In the proposed system, criticality is initially provided by constraint and objective agents and is propagated in the system through their requests.
Let's illustrate this with a constraint of the type \(g(X) \leq t\), with \emph{X} input of the constraint, \emph{g(X)} the constraint equation and \emph{t} the threshold under which the constraint is satisfied. The basic requirements regarding the criticality of this agent is to be low when the constraint is satisfied and high when the constraint is violated. Thus, the criticality of this agent is function of its current value and of the threshold.


\begin{figure}
	\centering
	\begin{subfigure}[b]{\textwidth}
	  \centering
	  \scriptsize
	  
		\[criticality_{t,\eta,\epsilon}(x)=
			\begin{cases}
				0		&\text{if \( x < \; t - \epsilon \) },\\
				-\gamma ( t - x - \eta ) ^ 2 / ( 2 ( \epsilon - \eta)) + \gamma ( t - x - \eta ) + \delta &\text{if \(t - \epsilon \leq \;x \leq \;t - \eta \)},\\
				\gamma (-t-x-\eta)^2/(2 \eta)+\gamma (-t-x-\eta) + \delta &\text{if \(t - \eta \leq \;x \leq \;t\)},\\
				1	&\text{if \( x > \; t \)}
			\end{cases}\]

		where\\
		$\gamma = -2/ \epsilon,$\\
		$\delta = -\gamma (\epsilon - \eta )/2,$\\
		$\text{and } 0 < \eta < \epsilon.$
	  \caption{Analytical formulation.}\label{crit_func}
	\end{subfigure}
	
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includegraphics[]{./latex_figs/barrer-fig}
		\caption{Shapes of criticality function of threshold t = 1 for \(\epsilon=1\) and different \(\eta\).}\label{crit_shapes}
	\end{subfigure}
	
\caption{Criticality function of a constraint agent.}
\end{figure}

To compute it, we use the function defined on \figurename{} \ref{crit_func}. It takes as input \(x\), the current value of the constraint. It is parameterized by \(t\), the threshold, and by \(\eta\) and \(\epsilon\) that both regulate the shape of the function as seen on \figurename{} \ref{crit_shapes}. Its value always varies between \(0\) and \(1\).
The \(\epsilon\) can be adjusted by a domain expert if needed: the higher it is, the faster the constraint increases in criticality.
In our experiments, we used $\epsilon = 0.1$ and  $\eta$ was set to roughly a third of $\epsilon$, \textit{i.e.} 0.03.
This function allows a smooth transition between two states and provides several interesting properties: it is continuous, differentiable, requires few parameters, is computed quickly and is relatively easy to grasp.

[[Faire référence aux fonctions barrières présentées auparavant ?]]

\noindent The criticality of the other agents is determined as follow:

\begin{compactitem}
\item For objective agents: the criticality is set to an arbitrary constant value which must be lower than \(1\). In our experiments we settled for a value of \(0.5\). This translates the fact that, in the general case, an objective could theoretically always be improved, but is less important to satisfy than a constraint.
\item For variable, output and model agents: the criticality is set to the highest criticality among the received requests.
\end{compactitem}

When the system converges to a solution, it stabilizes at a point where the maximum of the criticalities of the agents is minimized.
%%

An illustration on this mechanism can be seen on \figurename{} \ref{NCS_1_conflicting_requests}. In this example, a variable agent (with a current value of 3) receives contradictories requests from an objective (decrease) and a constraint (increase). As the constraint is not satisfied, the criticality associated to its request (0.85) is higher than the one of the objective (0.5). As a consequence the variable agent selects the request of the constraint agent and increases its value.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{NCS_1_conflicting_requests_a}
		\caption{The criterions send their requests and current criticalities (in parenthesis).}\label{NCS_1_conflicting_requests_a}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{NCS_1_conflicting_requests_b}
		\caption{The variable agent selects the most critical request and responds accordingly.}\label{NCS_1_conflicting_requests_b}
	\end{subfigure}
	
\caption{Criticality mechanism (the criticality of the requests is indicated in parenthesis).}\label{NCS_1_conflicting_requests}
\end{figure}

\subsection{Cooperative Trajectories}

\subsubsection{For Model Agents}

This NCS can be seen as an extension of the previous one (conflicting requests). It occurs when a model agent receives requests from its outputs which lead to contradictory changes on its inputs (\emph{Conflict} NCS). We presented in the previous section how using \emph{criticality} the model agent could discriminate between the requests by choosing to satisfy the most critical. However, in the specific case of the model agent, this mechanism is not the most efficient. Let us take a simple example, graphically represented in \figurename{} \ref{NCS_cooperatives_trajectories_models}.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{NCS_cooperatives_trajectories_models}
\caption{Cooperative trajectories for model agent example}\label{NCS_cooperatives_trajectories_models}
\end{figure}


Suppose the following problem:

\begin{align*}
\text{Minimize } &b_1\
\text{Maximize } &b_2\\
\end{align*}
Where $b_1,b_2$ are outputs of the model $M(a_1,a_2)$ defined as
\begin{align*}
b_1 = &2a_1 + a_2\\
b_2 = &a_1 + 2a_2\\
\end{align*}

Using conflicting requests mechanisms, the model agent $M$, upon receiving requests coming from the objective and constraint agents, would choose to fully satisfy the most critical request, disregarding completely the other. In this example, if the request of the first objective agent is the most critical, the model agent would ask $a_1$ and $a_2$ both to decrease, and would ask them both to increase if the second objective agent request was the most critical.\\
However we can easily observe how, in this specific problem, the input variables $a_1$ and $a_2$ has different impacts on the two objectives. A variation of 1 for the variable $a_1$ will change the first objective by 2 and the second one by 1, while a variation of 1 for the variable $a_2$ will have the inverse effect.\\
This observation leads to a third possibility: trying to satisfy both requests at once by propagating each request to the input agent(s) which is (or are) the most \enquote{influential}[[NOTE: try to find a synonymous, influence will be used elsewhere]] for each request. Doing so allows a most efficient exploration of the search space by improving several criteria at once.

[[Drawing of cooperative trajectory search space in 2D]]

Let $M(I,O)$ be a model agent with a set of inputs $I$ and a set of output $O$.

Let $\{r = <\Delta_{o_{\in O}}^{r}, crit>\}$ be a set of  requests received for outputs $\{o\} \in O$, where $\Delta_{o}^{r}$ is the variation of output $o$ requested by $r$ and $crit$ the criticality of the request. By hypothesis, at most one request can concerns the same output.
Using an external optimizer, the agent obtains for each $r_k$ a set of corresponding input variations $\{\Delta_i^{r_k}\}_{i \in I}$.

For each input $i$ of the model, we need to select a variation.
First of all, we only keep the requested variations with the maximum criticality:  $\{\Delta_i^{r_{max}}\} \leftarrow \{\Delta_i^r : crit_r = max \{crit_{r \in R}\}\}$.
If $|\{\Delta_i^{r_{max}}\}| =  1$, then the agent knows which variation to request to the input agent.

Otherwise, the model agent will discriminate using the relative influence[[influence is not a good term]] [[it learned | given by the analyser]] of the input on the outputs concerned by the requests: 
$\Delta_i^* \leftarrow \left\{ \Delta_i^{r_{max}} : r_{max} = argmax \dfrac{inf_{i,o_{r_{max}}}}{\displaystyle\sum_{i \in I} inf_{i,o}} \right\}$

If at this step $|\Delta_i^*| >1$ [[then what?]]. Otherwise the unique $\Delta_i^*$ is selected.

\begin{algorithm}
\caption{Cooperative trajectory - Model agent}
\label{algo_cooperative_trajectory_model}

	\ForEach{$i \in I_M$}{
		 $\{\Delta_i^{r_{max}}\} \leftarrow \{\Delta_i^r : crit_r = max \{crit_{r \in R}\}\}$\;
		 \eIf{$|\{\Delta_i^{r_{max}}\}| = 1$}{
		 	$\Delta_i^{r_{max}}$ is the variation requested to the input\;
		 }
		 {
			$\{\Delta_i^*\} \leftarrow \left\{ \Delta_i^{r_{max}} : r_{max} = argmax \dfrac{inf_{i,o_{r_{max}}}}{\displaystyle\sum _{i \in I} inf_{i,o}} \right\}$\;
		 }
	}

\end{algorithm}

\subsubsection{For Variable Agents}

We have seen in the previous section how it could be possible in some cases to propose an more efficient exploration of the search space by making model agents selectively propagate requests based on the influences[[OTHER TERM ?]] of theirs inputs on theirs outputs. This mechanism based itself on the fact that all the requests pass by a model agent, which is able to \enquote{dispatch} the requests to different inputs.\\
However, if we take the example problem, we can reformulate it as the following equivalent problem:

\begin{align*}
\text{Minimize } &2a_1 + a_2\\
\text{Maximize } &a_1 + 2a_2\\
\end{align*}

The graphical representation of this problem is shown in \figurename{} \ref{NCS_cooperatives_trajectories_variables}.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{NCS_cooperatives_trajectories_variables}
\caption{Cooperative trajectories for variables agents example}\label{NCS_cooperatives_trajectories_variables}
\end{figure}

Using this new formulation, no model agent is create during the MAS transformation of the problem, but the underlying problem between cooperative trajectories still exists but with an additional difficulty. As we already said, in order to be able to scale with the complexity of the problem, each agent must keep a local view. Consequently, the criteria agents are not aware of each others, and neither are the variable agents.\\
We will now show how the variables agents can become aware of the NCS and coordinate to solve it without communicating with, or even acknowledging, each other.

[[TODO: Describe mechanisms]]

\subsection{Cycle Solving}\label{NCS_cycles}

A common difficulty regarding complex optimization problem (and complex systems in general) is the existence of interdependencies (which can be thought as instances of \emph{feedback loops}). For example, for the design of an aircraft, one could try to increase the range of the aircraft by increasing its fuel tank. But doing so, it increases the mass of the aircraft, which in return decreases its range. This complex interdependency between range and mass will appears in the analytical formulation as a cycle in the calculus of the models, \emph{i.e.} to produces its outputs $O_M$, the $Mass$ model will need the outputs $O_R$ calculated by the $Range$ model, taking itself in input the outputs $O_M$ of $Mass$:
\begin{align*}
O_M = &Mass(O_R)\\
O_R = &Range(O_M)
\end{align*}

Such cycles in the problem formulation can pose severe challenge for the optimization process. Once more, we must keep in mind that models must be handled as black boxes; Consequently an analytical solving of the cycle is impossible. Consequently, it is the burden of the agents to detect cycle and take correcting actions of required.

[[Reprendre le passage sur les fixed point iteration du SoA]]

The most basic situation regarding such cycles concerns two directly interdependent models (i.e each one using an output of the other as its input). However, much complex cycles topologies may be encountered. For example, an arbitrary high number of models can be successively chained together in one giant multi-step cycle, or several models can present multiples interdependencies with each others.The solution for which such a cycle is stable (if it exists) is called the \emph{fixed point}.

As the system must work by iterating over these black boxes, the values of the outputs will be iteratively recalculated and propagated between the models involved in the cycle. Depending on the exact structure of the cycle, its effects on the problem solving can be radically different. To illustrate this, let's take the simple following example:
\begin{gather*}
y = a1 \times x \\
x = a2 \times y
\end{gather*}
where \emph{x}, \emph{y} are variables and \emph{a}1, \emph{a2} fixed parameters.
 
The aim is to find coherent values for \emph{x} and \emph{y}. Since models are black boxes, so we cannot directly infer the solution. Depending on the parameters \emph{a1} and \emph{a2} (and excluding the trivial case of \(a1, a2  = 0\)), this structure behaves differently: 
 
\begin{compactitem}
\item \(a1, a2 = 1\) : in this case any value for both \emph{x} and \emph{y} satisfies the problem, there is an infinity of fixed points
\item \(a1, a2 < 1\) : in this case the system will converge towards the solution. The fixed point is said to be \emph{attractive}
\item \(a1, a2 > 1\) : in this case the system will diverge in the opposite direction of the fixed point, which is said to be \emph{repulsive}
\end{compactitem}

The two first cases are benign, and do not impeded the solving process. The third case however will disturb the solving process by moving the state of the system further and further from the stable solution. The task of the agent is then twofold:
\begin{compactenum}
\item detect existing cycles
\item if a cycle is currently diverging, take a correcting action to ensure it converges
\end{compactenum}

An example of diverging cycle is shown in \figurename{} \ref{NCS_cycles}

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{NCS_cycles}
\caption{Diverging cycle example}\label{NCS_cycles}
\end{figure}

[[TOFIX: will probably be outdated + must write the synthesis algorithm]]
To address the first point, each message is uniquely signed to register its origin. When an agent initiates a message sending by itself (i.e. not as the result of a received message), it \emph{signs} the message with its unique agent "ID" and an unique message number. The association of these two elements is the \emph{origin signature}.

When an agent sends a message in response to a received message, it adds to the sent message the origin signature of the message causing its action. This way, the signature origin is preserved from message to message and can be used to pinpoint the origin of an activity of the system.

The \emph{Output agents} will be in charge of detecting and handling cycles, as they are in the best position for this. Indeed, being at the junction between criteria and models (or between different models), they are often the ones that can detect cycles the earliest.

To detect a cycle,  the output agent creates a correspondence table associating for each origin agent the last signature it received from it. When receiving a message, the agent checks its origin. If the origin agent is not present in the table, the agent adds it. If it is present, it compares the new signature with the one it memorized. If the signatures are different, the agent replaces the old signature with the new one (as the new signature corresponds to a more recent request). If the two signatures match, then there is a cycle.

In case of a cycle, the output agent must now determine if the cycle is converging or diverging towards the fixed point. As in the general case all models are black boxes, the output agent needs to observe the evolution of output values while iterating through the cycle. If the difference between successive values is decreasing, the cycle is converging towards the fixed point, else the cycle is diverging as the variables are going in the "wrong direction". To this end, the output agent memorizes the difference between its old and new value, and continues as usual. When receiving the next message from the cycle, it calculates the new difference which enables it to decide if the cycle is converging or not towards the fixed point.

If the cycle is converging, the output agent does not need to do any special action. But in case of a diverging cycle, instead of taking the requested value, the agent emits to its model a request for the opposite direction (ie, if the value of the output should have increased of \emph{n}, the agent will request a new value corresponding to a decrease of \emph{n}). The request will propagate through the cycle until the fixed point is found.
[[END TOFIX]]

\subsection{Hidden Dependencies}\label{hidden_deps}

The fact that each agent has only a local and limited view of the problem is a requirement for the system to be able to scale to arbitrary large problems. However this restriction can prove problematic when, due to its limited perceptions, an agent makes a wrongful assumption regarding its neighbors. The \emph{hidden dependency} NCS happens when an agent considers that the requests its makes to its input agents are independents while the two agents are in fact dependents. The mistake of the requesting agent comes from the fact that the dependency link between its two input agents is beyond its perception range.

\figurename{} \ref{NCS_hidden_dependencies} is an illustration of a configuration which will lead to a hidden dependency NCS. In this example, the objective agent $\text{min }a_1 + a_2$ will assume that the variable agents $a_1$ and $a_2$ are independents, while the value of $a_2$ is in fact completely dependent from the one of $a_1$ (more precisely $a_2 = -\frac{1}{2} a_1$). In this context, the objective agent will send separate requests to the two variables agents, asking them both to decrease. Since $a_2$ is an output variable, it will transmit the request to its model agent, which will in turn transmit it to $a_1$. And since $a_2$ and $a_1$ are negatively correlated, the transmitted request will ask $a_1$ to increase. Consequently $a_1$ will receive two requests, one (directly from the objective) asking it to decrease, and one (transmitted by $a_2$⁾ asking it to increase.

\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{NCS_hidden_dependencies}
\caption{Hidden dependency example}\label{NCS_hidden_dependencies}
\end{figure}

This situation is clearly a NCS, as two requests having the same origin should not require an agent to do contradictory actions. We will now propose a mechanism to allow the variable agent $a_1$ to solve the NCS efficiently.

First of all the agent must identify the NCS. This part proves to be quite easy using the \emph{origin signature} information we introduced in section \ref{NCS_cycles}. Since every message contains a field recording which agent was at the origin of the request, any agent can know if two requests originate from the same agent by comparing this origin field.\\
In our example, the agent $a_1$ is able to easily detect the NCS, by comparing the two contradictory requests and observing that their origins are the same.

However, our agent still does not know how to solve the NCS as the requests, originating from the same criterion, have identical criticality. To understand what is needed for the agents to be able to solve the problem, let us examine our example again. In our example, the solution is quite obvious. We want to minimize $a_1 + a_2$, knowing that $a_2 = -\frac{1}{2}a_1$. This problem is then equivalent to minimizing $a_1-\frac{1}{2}a_1$ or, simplified, $\frac{1}{2}a_1$. The correct behavior is thus for $a_1$ to decrease.\\
Let us now change a little this problem, considering instead $a_2$ to be equal to $ -2a_1$. In this case the problem can be rewritten as (after simplifying) minimizing $-a_1$, or maximizing $a_1$. In this case the correct behavior for $a_1$ would be to increase.\\
It is now obvious that the correct behavior for the agent $a_1$ is extremely dependent on the coefficient of the intermediate model. The same can be said about the coefficients of the objective, and could be said in the general case about any of the intermediate models which could happens to be between the objective and the variable agents. Conceptually, we could say that the variable agent is linked to the objective agent by different \enquote{branches}, with each branch having a specific influence on the objective. In our original example the two \enquote{branches} would have influences of respectively $+1$ and $-\frac{1}{2}$ (and $+1$ and $-2$ in the modified example). That is, the impact of a change of $a_1$ of $\delta_{a_1}$ is of 1 considering the direct branch, and of $-\frac{1}{2}$ considering the branch passing by $a_2$, for a total influence of $\frac{1}{2}$.\\
If we can provide this information to the variable $a_1$, then the agent will be able to solve the NCS by selecting the request coming from the most influent branch(es), ensuring it action will be beneficial for the origin of the requests. We will now see how, by adding a new information with the requests, the agents are able to propagate their local influence to the variable agent, allowing it to solve the NCS.

\begin{algorithm}
\caption{Influence propagation by internal model agents}
\label{algo_hidden_dependency_influence_propagation}
	\SetKwProg{Bv}{behavior of }{}{}

	\Bv{Objective/Constraint Agent}{
		${c_i} \leftarrow correlations(Inputs, \{output\})$\;
		\tcp{constraint and objective agents have one output only}
		\ForEach{$i \in Inputs$}{
			$influence_{r_i} \leftarrow c_i$\;
		}
	}
	
	\Bv{Model Agent}{
		${c_{i,o}} \leftarrow correlations(Inputs, Outputs)$\;
		\tcp{model agents can have several outputs}
		\ForEach{$r \in$ {selected requests}}{
			\ForEach{$i \in Inputs$}{
				$o \leftarrow sender_r$\;
				$influence_{r_i} \leftarrow influence_r \times c_{i,o}$\;
			}	
		}
	}

\end{algorithm}

Before presenting the exact propagation mechanism, we need to make a quick clarification. In our example, we reasoned on linear models by looking at their coefficients in order to know the different influences. Such reasoning could not be applied to the general black boxes models that our agents use. However, we presented in section \ref{model_agent_solving} a general algorithm which allows the agents to create local linear approximations of any black box models by observing the correlations between the inputs and the outputs of the internal model. Consequently we can assume that each agent with an internal model can always extract the linear factors corresponding to the model, either because they were given to them by the designer, or because they used the aforementioned method to create a local linear approximation (using the observed correlations). During the rest of this section, we will assume that the agents apply the estimation algorithm and use the observed correlations as approximate linear factors of the problem.

To provide the variable agent with the correct influence information, the agents use a simple propagation mechanism. An \emph{influence} field is added to the requests transmitted into the system. The criterion agent which send the original requests fill the influence information with the observed correlation between its output and and the input corresponding to the agent to which the request is send. An intermediate variable agent will propagate the request without modifying this field, while a model agent will replace it by its old value \emph{multiplied} by the correlation its observed between the output from which it received the request and the input to which the request is send (this operation is done independently for each input if a request is propagate to several inputs of the model). When a variable agent detect a hidden dependency NCS, it separate the requests coming from the origin in two groups: the requests asking the variable to increase, and the requests asking the variable to decrease. For each group the agent calculates the sum of the influences of each requests in the group, then compare the \emph{absolute values} of the total influences. The agent will select the action corresponding to the biggest influence (in absolute value). If the agent is a design variable, it will change accordingly, if it is an output variable it will propagate a request with the same origin and information, but replacing the influence value with the sum of the influences of the winning group.\\
We can say that, in the same way that we \enquote{collapsed} the intermediate models when we wanted find the solution to our example problem, the intermediate correlations are \enquote{collapsed} into the influence information for the variable agent to be able to solve the NCS.

\begin{algorithm}
\caption{Hidden dependency detection and solving by variable agents}
\label{algo_hidden_dependency_solving}

	$R_{received} \leftarrow $ { received requests}\;
	$R_{selected} \leftarrow \{ r \in R_{received} : \operatorname*{arg\,max}_{r \in R_{received}}(criticality_r) \}$\;

	\If{$|R_{selected}| > 1 \wedge \forall( r_1, r_2 \in R_{selected}: origin_{r_1} = origin_{r_2})$}{

		$R_- \leftarrow \{r \in R_{selected} : value_{r} < value_{current}$\}\;
		$R_+ \leftarrow \{r \in R_{selected} : value_{r} > value_{current}$\}\;
		
		\If{$R_- \neq \emptyset \wedge R_+ \neq \emptyset$}{
			\tcp{A hidden dependency NCS is detected}
			$Inf_- = \displaystyle\sum_{r \in R_{-}}{influence_r}$\;
			$Inf_+ =\displaystyle\sum_{r \in R_{+}}{influence_r}$\;
	
			\eIf{$|Inf_-| > |Inf_+| 1$}
			{
				$R_{selected} \leftarrow R_-$\;
			}
			{
				$R_{selected} \leftarrow R_+$\;
			}
			apply $R_{selected}$\;
		}
	}

\end{algorithm}

\subsection{Asynchronous Messages}

The last NCS addresses an underlying assumption we have made until now. When we described the behavior of the agents, we supposed each of them always receive the messages in a timely manner, waiting if necessary until all the relevant messages are transmitted before acting. Once again, this hypothetical scenario is made impossible by the need for the agents to maintain a local view. The agents do not know what messages are currently transiting in the system. Their only knowledge concerns the messages they received at the start of their cycle and what messages they previously sent (but not their current status). Consequently, the topology of the agent graph can lead to transmission delays, where messages sent at the same time by the same agent will not necessarily reach their ultimate destination at the same time. For example, let us look back at the hidden dependency example (reporduced on \figurename{} \ref{NCS_hidden_dependencies} for convenience). When we discussed the case of hidden dependencies, we assumed by all the messages sent by the objective agent would reach $a_1$ before the latter would take its decision. However, it is clear that one of the message will take more time to reach $a_1$ than the other, as it must transit by two intermediate. Why would the agent $a_1$ wait for this late message, without even knowing its existence, before acting?

\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{NCS_hidden_dependencies}
\caption{Hidden dependency example (reproduced)}\label{NCS_async_requests}
\end{figure}

Many of the previous mechanisms we proposed rely on a timely information delivery, for example for the agents to be able to detect some NCS. Based on this, it ensues that, without an adequate \enquote{synchronization} mechanism, many of them would fail to work correctly. On the other hand, having a fully synchronized system, where all the agents would make lengthy checks to ensure the complete propagation of all messages to all recipient, would be undesirable. Indeed, complex continuous optimization problems create big but loosely connected graphs, for which such full synchronized mechanism would be both costly and inefficient. Moreover, if we take in account the dynamic aspects of the problem (having a designer which can add or remove agent at any moment during solving), obtaining such synchronization guarantee can be nigh impossible.

In this section we propose a somewhat middle ground mechanism, where the agents try to estimate if they received enough messages to make a correct decision. As some of the information they require is learned during the solving process, an adaptation period will be necessary in order for the agent to behave correctly. During this period the agents may have a suboptimal behavior where some NCS are not be correctly detected. The iterative nature of the solving process allows these suboptimal decisions to be ultimately corrected by the agents themselves.

The solving mechanism for this algorithm is composed of two parts, regarding how the agents make their estimations for respectively the request messages or the inform messages they expect to receive.

Concerning the requests they expect to receive, the agents base their reasoning on the fact that requests are only sent in answer to previously received inform messages. Consequently they except an answer for the inform messages they previously send. So the agent assume at least one request message by output agent to which it send an inform message. As the agents are cooperative, each output agent should forward at most one request (the one deemed the most important to handle) back to the agent. Consequently, an agent can expect to receive one and only one request message in response to the informs it send. Once it received a request message from each of its output agents, it can take its decision (either forwarding the most important request to its input(s) or, in the case of a design variable, changing its value and sending new inform messages).\\
The only modification which need to be done for this mechanism to work concern (non-criterion) agents which are not linked to any output agents. Previously, when receiving inform messages, these agents would change their value accordingly, but would not send back any message (as they have no request to do). In order for the \enquote{upstream} agents not to be blocked waiting for an answer, an agent that neither has requests to send or informs to forward must answer to its input agent with an \emph{allgood} message, signaling that it has taken in account the inform message and that non other action is pending. An agent receiving such allgood message know that it no longer need to wait for this output for taking its decision.

In the same way that an agent waits for requests messages after having send informs, an agent which has send request messages should wait to receive enough relevant inform messages before taking further steps. In the same way that an agent sending informs expects to receive request messages in return, an agent sending requests expects to receive inform messages in response. A basic solution is thus to apply the same algorithm. In this case however, the agent is not strictly required to wait for all the inform messages to take a relevant decision. We presented in section \ref{model_agent_solving} how the internal model agents can get an indication of the impact of their inputs in regard of their outputs using correlation information. Consequently, such agents can wait until they estimate that they received informs from their inputs corresponding to a total of more than 50\% of the total \enquote{impact} on the value of each output (variables agents, having only one input, always have to wait for this input and this input only, which correspond of 100\% of the total \enquote{impact} on its value) .


\begin{algorithm}
\caption{Waiting algorithm for request messages}
\label{algo_waiting_requests}

	$O \leftarrow $ \{ outputs \}\;
	$Requests \leftarrow $ \{ received requests \}\;
	$AllGoods \leftarrow $ \{ received allgoods \}\;
	$stillwaiting \leftarrow |Requests| + |AllGoods| \neq |O|$\;

\end{algorithm}

\begin{algorithm}
\caption{Waiting algorithm for inform messages}
\label{algo_waiting_inform}

	$O \leftarrow $ \{ outputs \}\;
	$Informs \leftarrow $ \{ received informs \}\;
	$stillwaiting \leftarrow \forall output \in O,  (\displaystyle\sum_{info \in Informs}{ correlation(input_{info}, output) \leq 50\%})$\;

\end{algorithm}

In our implementation, we did not concern ourself with potential loss of messages, which can happens when the designer intervenes on the system by removing existing agents while messages are transiting. For a \enquote{production-ready} version of the algorithm, such potential losses should be taken in account. Otherwise some agents may hang indefinitely waiting for  lost messages. A simple way to remediate to this problem is to add timeouts to the agents. Timeouts are a well-studied problem in computer networks, and adequate algorithms can be proposed based on the works in the existing literature (see \cite{jacobson1988congestion} for example).

\subsection{Summary of Non-Cooperative Situations}

In this section we presented several problematic configurations (NCS) which can arise from the specificities of our modeling of a continuous optimization problem. For each difficulty we presented a basic example of problematic configuration and how this configuration would cause a cooperation failure with the nominal behavior of our agents. The different NCS are summarized in \tablename{} \ref{NCS_summary}

\begin{table}
\caption{Non Cooperative Situations Summary}\label{NCS_summary}
\centering
\begin{tabular}{|c|c|p{5cm}|p{3.5cm}|}
	\hline
		NCS & Category & Description & Cooperative Mechanisms\\
	\hline
	Conflicting Requests & Conflict & An agent receives several incompatible requests & Criticality\\
	\hline
	Cooperative Trajectories & Conflict & An agent receives seemingly incompatible requests, which can each be satisfied & [[??]]\\
	\hline
	Cycle Solving & Conflict & Several agents are involved in a diverging cycle  & sender timestamps, origin timestamps\\
	\hline
	Hidden Dependencies & Conflict & An agent sends seemingly independent requests to  dependent agents & origin timestamps, influence\\
	\hline
	Asynchronous Messages & Conflict & Agents receive messages in untimely manner & influence\\
	\hline
\end{tabular}

\end{table}
[[TODO complete the table]]

In each case, we proposed general mechanisms for the agents to be able to detect and correct the NCS in order to maintain a correct and efficient optimization process. In general, we had to provide enough information for the involved agents to detect and correct the NCS. The different mechanisms are summarized in \tablename{} \ref{NCS_mechanisms_summary}

\begin{table}
\caption{Non Cooperative Solving Mechanisms}\label{NCS_mechanisms_summary}
\centering
\begin{tabular}{|c|p{3.5cm}|p{3.5cm}|p{5cm}|}
	\hline
		Mechanism & Description & NCS Involved & Properties\\
	\hline
	Criticality & A aggregated measure to indicate the state of the agent & Conflicting Requests & Comparable between different agents\\
	\hline
	Sender/Origin Timestamps & A unique signature composed of the unique id of the sender/origin of the message and a (local) time step & Cycles Solving, Hidden Dependencies, Asynchronous Messages, [[Cooperative Trajectories ?]] & Comparable by sender/origin, allows a partial ordering of the messages (by sender/origin)\\
	\hline
	Influence & An indicator of the impact value of the receiver on the origin & Hidden Dependencies & Comparable by origin\\
	\hline
	 [[Participation ??]]& & & & 
	\hline
\end{tabular}
\end{table}

\subsubsection*{Complexity Analysis of Solving Mechanisms}

We now provide a quick complexity analysis of the different solving mechanisms.

\paragraph*{Conflicting Requests}

The main work for solving this NCS consists in finding the request(s) with the highest criticality. This very simple operation can be done by examining each received request in sequence and have a complexity of $\mathcal{O}(n)$ in the number of requests.\\
The solving of this NCS does not requires sending additional messages

\paragraph*{Cooperative Trajectories - Models}

To solve this NCS, the model agent must, for each input, chose the selected request for which the input is the most important. The complexity of the algorithm is thus $\mathcal{O}(n \times i)$, $n$ being the number of selected requests and $i$ the number of inputs.\\
When solving this NCS, the model agents sends the same number of messages then in the nominal case.

\paragraph*{Cooperative Trajectories - Variables}

[[TODO]]

\paragraph*{Cycle Solving}

[[TODO]]

\paragraph*{Hidden Dependencies}
When solving this NCS, the agent needs to detect which selected requests come from the same origin, group them, before comparing the total influences of the two groups. The detection, grouping and total influence calculation can all be factorized into a single pass algorithm of complexity $\mathcal{O}(n)$ regarding the number of requests.\\
The agent will not need to send additional messages for solving this NCS.

\paragraph*{Asynchronous Messages}

The algorithm for waiting request is trivial and can be executed in constant time. The algorithm for waiting informs requires a study of every inform for each output, its complexity is thus $\mathcal{O}(n \times o)$, $n$ being the number of informs and $o$ the number of outputs.\\
This mechanism requires the sending of additional messages from (non-criterion) output agents which themselves do not have any output agent. In the best case, there is no such agent, thus no additional message is sent. In the worst case all outputs agent are concerned, and need each to send one additional message (message complexity of $\mathcal{O}(o)$, $o$ being the number of outputs).

\paragraph*{}
It can  be seen that, overall, the cooperative mechanisms we proposed to solve the various NCS are quite reasonably efficient in their complexity. This property is important in order for the system to be able to scale to bigger and more complex optimization problem. We managed to keep the complexity of the algorithms low by keeping the reasoning of the agents to a local level, limiting their perceptions to their neighborhood and only very synthetic information about the rest of the system (criticality, origin \emp{etc.}). Moreover, it should be noted that, in many problems, some of the agents will only have a minimal role in the solving, being connected only to one agent in input and one agent in output. For these agents, the complexity of these mechanism will be minimal as they will always receive at most one message. Only in the utmost complex and coupled problems will the majority of the agent be involved in the solving of NCS.

A concern could be raised about the composition of these different mechanisms. Indeed an agent can be involved in several NCS at once. In this case, would not the combination of these mechanisms cause exponential increase of complexity? While this concern could be correct in theory, in practice the different solving mechanisms play the role of successive filters regarding the received messages. Consequently, later mechanisms work on a far lower number of messages than the earlier ones. The typical example is the conflicting requests mechanisms, which will eliminate all the requests which are not deemed critical enough. The others mechanisms will only have to take in account the requests which were conserved by this first filter. More so, it should be noted that, while the different mechanisms have be presented separately, a large part of the processing they do on the messages can be factorized, in order to increase the overall efficiency. While in our implementation we did not do such optimization (in order to keep the mechanisms separated and code easier to work with), such improvements would be of course strongly recommended for a more \enquote{production-ready} implementation of the system.\\
To conclude on this remark, we can safely say that the composition of the different cooperative mechanisms is not of additive complexity, but only bring a marginal increase in the total complexity of the agent behavior.

\chapter{Extending the Multi-Agent System for Uncertainties}

We have seen in section \ref{SOA_uncertainties} how uncertainties can be a major concern in real-life optimization. We have also seen how several types of uncertainties existed and needed to be taken into account. Moreover, complex optimization problems can involve several teams concerned with different disciplines, each of which with its own practices regarding how to manipulate uncertainties, making difficult to study the impact of uncertainties across disciplines.

In this chapter we present a way to alleviate this burden by having the agents automating the uncertainty propagation in the system.

The difficulty is twofold:
\begin{compactitem}
\item \textbf{How do the agents handle uncertainties ?}
How does a model agent calculate the uncertainties on its outputs from the uncertainties of its inputs and its internal model ? How does a constraint agent can handle a probabilistic constraint ?

\item \textbf{How do the agents combine heterogeneous uncertainties modeling ?}
How can a model agent handle inputs with different uncertainties modelings ?
\end{compactitem}

To answer these difficulties we will present a general mechanism inspired by the one we used with external optimizers, which is based on the use of encapsulated external tools to apply the domain-specific expertise.

\section{From Deterministic Optimization to Optimization under Uncertainties}

[[Reformulation d'un probleme d'optim] -> cf these_Vicent_Baudoui_Optimization_robuste]]

We previously explained that uncertainties can be present at several levels in the optimization problem. We consider here the two following types of uncertainties:
\begin{compactitem}
\item \textbf{Variable uncertainties}, which are brought by the fact that we cannot perfectly control the value which will be effectively assigned by the design variable.
\item \textbf{Model uncertainties}, which represent the limited precision of the models.
\end{compactitem}

Taking in account these uncertainties leads to a modification of the criteria formulation. Indeed, the constraint $x < 10$ makes sense in the context of deterministic optimization, but not so much when the value of $x$ is uncertain. Is the constraint satisfied when $x$ is an uncertain value, for example represented by an interval ranging from 8 to 12?\\
One can instead propose the following probabilistic constraint $P(x < 10) > 90\%$, which can still be evaluated when manipulating uncertain values. It is possible to propose others criteria formulations manipulating uncertain values, for example one could consider a deterministic constraint concerning the mean of an uncertain variable. Usually, constraints are expressed in a probabilistic form, while objectives use the mean of the variables[[NEED TO REF THIS ?]].

[[Give an example of transformation of a deterministic problem to a non deterministic one]]

\section{Manipulating Uncertain Values}

[[Bien expliquer que la surcharge des operateurs ne concerne que les agents, et non pas le contenu des boites noires]]

Taking in account uncertainties requires modifications at several levels. The most obvious concerns the data structure. On the deterministic version, the data exchanged by the agent were simply real values \footnote{With the obvious limitation of the representation of real numbers on a machine.}. To be able to exchange uncertain values, the agents require a more complex data structure.

As we presented earlier, there are many ways to represent uncertainties, each of them having specific requirements. For example, an interval representation requires two values, the minimum and maximum boundaries, while an uncertainty expressed by a set of measures obtained with Monte Carlos experiments can require the storage of hundreds of thousands points. As we want to maintain the possibility for the designers to choose how they represent uncertainties, we need to provide an abstraction for the agents to be able to consistently handle uncertain values whatever the specific representation [[CORRECT ENGLISH?]].

To this end, we specified the minimum set of operations the agents require. To be able to manipulate an uncertain value, an agent needs to be able to :
\begin{compactitem}
\item obtain the mean of the value (or at least a representative value).
\item obtain the standard deviation of the uncertainty.
\item draw a random value following the law represented by the uncertainty.
\item obtain the cumulative probability of a value (\emph{i.e.} for a given value $t$, what is $P(X < t)$) 
\item obtain the inverse cumulative probability (for a given probability $p$, what is $t$ for which $P(X < t) = p$) associated with the uncertainty).
\end{compactitem}

Any uncertainty representation which can provide these five informations can be used by the system. All the designer needs to do is to provide the module which implements these operations\footnote{The reader familiar the concepts of object-oriented programming and interface implementation will have no difficulty to guess how the process is done in our prototype}.

Let us see how one can provides these information regarding two drastically different modelings: intervals and Monte Carlo experiments.

\paragraph{Intervals Representation}

As the reader knows, an interval representation consist of a lower and upper boundaries. It can be considered as an uniform law, with all the values between the boundaries considered as equally likely. Suppose $l$ and $u$ the boundaries of our uncertainty, we can obtain the required informations using well-known formulas for the uniform law.

\begin{compactitem}
\item the mean can easily be obtained as $\dfrac{u + l}{2}$.
\item the standard deviation is $\sqrt{(u - l)^2 / 12}$.
\item the drawing of a random value consists of drawing a value between $l$ and $u$ using an uniform law, which presents no difficulty.
\item the cumulative probability of $t$ is calculated as $P(X < t) =
			\begin{cases}
				0	&\text{ if t < l}\\
				 \dfrac{t - l }{u - l}	& \text{ if } l < t < u\\
				1	&\text{ if t > u}
			\end{cases}$
\item the inverse cumulative probability of $p$ is obtained as $t = l + p (u - l)$.
\end{compactitem}

As we can see, for a simple representation as intervals the required informations can be obtained very easily.

\paragraph{Monte Carlo Experiments}

The Monte Carlo Experiment is not an uncertainty law \emph{per se} but a series of random drawings to obtain experiment points. If the number of drawings is big enough, then the series can provide an adequate information about the uncertainties governing the variable. Suppose $\{x_n\}$ a series of $n$ drawings over $x$. We suppose the values to be ordered (that is, $x_i < x_j$ 

\begin{compactitem}
\item the mean can be obtained with $\dfrac{\displaystyle\sum_n{x_n}}{n}$.
\item the standard deviation is obtained by first calculating the variance using the formula $Var(X) = E(X^2) - E(X)^2$ (possible as we can calculate the mean), then taking the square root to obtain the standard deviation.
\item the drawing of a random value consists of drawing a value $i$ between $1$ and $n$ and returning the corresponding point $x_i$
\item the cumulative probability of $t$ is calculated as  $P(X < t) = \dfrac{|\{x_i, \forall x_i < t\}|}{|\{x_n\}|}$ (where $|\{x\}|$ is the cardinality of the set $\{x\}$), \emph{i.e.} the number of elements lower than $t$ divided by the total number of elements.
\item the inverse cumulative probability of $p$ is obtained as $ t = $ [[TODO]].
\end{compactitem}

Even with very specific, non-analytical representation of uncertainties such as datasets from Monte Carlo experiments, we can provide the needed informations without much hassle.

Now that we have defined a common set of operations common to all the uncertainties representations, we can adapt the agents behavior in order to handle them. Such \enquote{uncertainty container} can be associated to variables to represent uncertain values, or to models, indicating uncertainties concerning the underlying model.

For example the designer can estimate that, due to physical limitations, the value of a design variable $x$ cannot be exactly be controlled and will always suffer from an imprecision of +/-0.1 around its assigned value. He can model the uncertainty around the variable with an interval. If he decides to arbitrarily initialize the variable at the value 8, the uncertain value of $x$ will then be [7.9; 8.1].

If the designer observe that the real-world process represented by a specific model tends to produce outputs whose values imprecision can be accurately modeled by a normal distribution, he can associate the normal distribution uncertainty to the model. For example the model $y = 2x$, upon the reception of the information $x = 1$, will produce an uncertain value $y$ with a mean of 2 and a variance corresponding to the associated normal law.

We have now seen how the agents can manipulate uncertainties in isolation. But these mechanisms do not allow the agents to propagate uncertainties in the system. Indeed, if we take our two previous examples, what if the variable $x$ ,associated with an interval uncertainty, is assigned in input of the model $y = 2x$, following a normal distribution? The model has no mechanism to handle uncertainties \emph{received on its inputs}, but these uncertainties must be taken into account as they surely have consequences on the uncertainties of its outputs.\\
The difficulty here is twofold. Not only the model agent must be able to take in account the uncertainties provided by its inputs in addition to its own uncertainty, but it must be able to do so even with uncertainties of different kinds (in our example, interval and normal distribution). We will now present a generic mechanism using \emph{uncertainties propagators} which allows to solve both problems at once.

\section{Uncertainties Propagators}

[[Need to illustrate how the propagator is used at runtime (diagram)]]

In the same way we encapsulated expert knowledge on numerical optimization using external optimizers, we propose to encapsulate the knowledge related to the propagation of uncertainties into dedicated modules we call \emph{uncertainties propagators}. While external optimizers are used by the agents to solve local optimization problems, the uncertainties propagators are used by the agents to propagate the uncertainties associated with their inputs and model into uncertainties on their outputs.

The role of an uncertainties propagator is to determine the uncertainties associated with the outputs values of the model based on:
\begin{compactitem}
\item the model itself
\item the uncertainty associated with the model
\item the input values
\item the uncertainties associated with the inputs
\end{compactitem}

As the outputs values cannot be determined by simply evaluating the (deterministic) model, the agent in charge of it will delegate the whole evaluation process to the uncertainties propagator when working with uncertainties. From the agent point of view, an uncertainties propagator is a black box function taking uncertain input values and an uncertain model, and producing uncertain outputs values.

\begin{figure}
\center
\includegraphics[width=0.4\textwidth]{uncertainties_propagator}
\caption{Uncertainties propagator as a black box function.}\label{uncertainties_propagator}
\end{figure}

\figurename{} \ref{uncertainties_propagator} illustrates the representation of an uncertainties propagator from the point of view of an agent. The uncertainty propagator is a black box. The $(i_j, u_{i_j})$ represent the $n$ uncertain input values of the model (the $i_j$ corresponding to the mean value of the input and $u_{i_j}$ to the associated uncertainty). The model itself $m$ is also provided along with its associated uncertainty $u_m$. Based on the inputs and the model (and their uncertainties), the uncertainty propagator returns the $(o_j, u_{o_j})$ represent the $k$ uncertain output values produced.

This approach shows several advantages. First it allow to keep the functioning of the agents nearly untouched, which in itself is a good sign. Handling uncertain values can be deemed to be an orthogonal concern to the previous ones, and orthogonal concerns should lead to orthogonal changes. Thus this modification of our system to handle uncertainties show that \emph{our modeling is flexible and can be extended easily}.\\
This approach also permit to reduce the work of the designers, which only need to \emph{define the needed propagators once and reuse them} on several problems. This aspect is particularly important as defining a correct way to propagate uncertainties across different disciplines can be an extensive work involving experts from different domains. Providing a way to encapsulate the result of this work in a reusable component leads to a more efficient work process (this concern is especially recognized by software engineers, which created the well-known acronym: DRY - \enquote{Don't Repeat Yourself}).\\
Finally, by strictly encapsulating the required knowledge, we provide a highly modular and adaptable mechanism. \emph{If the needs regarding the modeling of the uncertainties changes, the consequences on the problem are limited solely to the uncertainties representation and/or propagators}. For example, if a designer needs to change the uncertainties associated with a model, he only has to change the uncertainty representation of the model and to use the appropriate propagator (or create it if no adequate propagator already exists).

[[Dire que les propagateurs peuvent être très génériques et très réutilisables, ou au contraire extremement specifiques et adhoc => transition sur la partie suivante]]


\subsection{Examples of Propagators}

[[TODO]]

\section{Conclusion on Uncertainties Handling}

We have seen how the agents can automatically propagate uncertainties in the system, relieving the designers of a part of the burden.\\
This extension to our modeling allow to take in account not only values but uncertain values, that is, values with an associated uncertainty representation. As the exact representation does not concern the agent, these uncertainties are mostly handled as black boxes. In a similar way to how they use optimizers to propagate requests with black box models, internal model agents can use uncertainties propagators to locally propagate uncertainties. In this regard, uncertainties propagators (as well as external optimizers) can be seen as a way to extend the capabilities of the agents provided by the users.

While this mechanism require for the users to provide the uncertainties propagators, it has the advantage that this specific effort has to be done only once, as the resulting propagator can be reusable at will. In this way it can greatly reduce the time different teams need to spend together to overcome the difficulties induced by different working practices. It also open the possibility of propagators libraries, which can be shared and imported as needed.

A note of warning however, the representation of uncertainties is inherently imprecise. In the same way that the modeling of a system contains imprecision, so does the modeling of the uncertainties used to represent this very same imprecision (and so would do the uncertainties used to represent the imprecision on the uncertainties...). Like discussed before, this imprecision is an inherent limitation of the real world. As the uncertainties modeling is in itself factor of uncertainties, it follows that the transformation between two different uncertainties representations will in itself contains some part of imprecision (if only because the passage from one representation to another can lead to information loss, \emph{e.g.} passing from a normal law to an interval modeling).\\
This problematic is not specific to our technique and is indeed  present all the same if the uncertainties propagation is done \enquote{manually}. But it is worth reminding, as this complete automation of the propagation could hide it from the mind of the designer.

As a remark we have seen how, by overloading some common mathematical operators, it was possible for the agents to manipulate uncertain values as if they were simple numerical values. Such work could be possibly done with complex numbers, matrix or others, arbitrarily complex, data structures, as long as the corresponding overloaded operators are provided.

\chapter{Synthesis of the Contribution and Perspectives}

In this part we presented a novel approach for complex continuous optimization using a multi-agent system do distribute the optimization process. We started our work with a new way to model a continuous optimization problem as an agent graph. This modeling allows for an easy and potentially automatic transformation of any problem as a graph, without requiring special knowledge or expertise regarding MAS. Moreover, this transformation does not requires any specific reformulation of the problem,  which can keep the natural formulation corresponding to its domain. Because of these properties we named our modeling Natural Domain Modeling for Optimization (NDMO).
\\
To the best of our knowledge, this is the first proposal for providing a common modeling of continuous problems as agents graphs. While we cannot guarantee that is specific modeling will be adopted, it is surely a first step toward opening the field to other contributions. 

Based on this modeling, we proposed a simple nominal behavior where the agents try to solve individual goals and only has a limited, local perception of their neighbors. These agents can interface with external optimization techniques to solve local optimization problems (or alternatively use some internal optimization mechanisms), and exchange request and inform messages in order to keep a global coherence of the solution.
\\
While our agents are already able to interface with external optimization methods, some refinements could be proposed. An example could be to, instead of providing each agent with an adequate optimizer, providing all the agents with an optimizer catalog where several optimization methods are presented and characterized. Based on their observation of the manipulated models, the agents could select the optimization method they deem the most appropriate, and even switch between different methods during the solving. Such mechanism would contribute even more to relieving the burden of  the designer, but would require to be effective an extensive study of the ontological properties of the field. An other improvement could concerns the internal optimization mechanisms we presented in \algorithmname{} \ref{algo_solving_internaloptim}. It could be possible to replace our simple linear approximation estimation by more astute techniques. For example, existing works concern  [[Parler travaux de Luc, Valérian etc.]]. Such methods could provide more precise estimations to be used by the agents.

This nominal behavior being insufficient to solve some of the issues brought by the specificities of continuous optimization, we use the AMAS theory to identify a set of non cooperative situations, which represent specific patterns of configurations where the nominal behavior of the agents result in a sub-optimal handling of the optimization process. For each of these NCS, we proposed cooperative mechanisms which enrich the nominal behavior of the agent. These additional mechanisms allow to, when necessary, detect and identify the NCS, and to do corrective actions in order to re-establish the correct optimization process.
\\
The AMAS theory has proved to be an effective guide to the conception of our system. By maintaining a local view and concentrating on specific problematic situations, we obtained a behavior which is both scalable and modular. An interesting aspect of this work concerns the different NCS we identified. As our application domain is in itself quite abstracted and general, the NCS we encountered seems themselves to be of a more general nature than in some other works. In part \ref{CPSP} we will explore possible uses of this work to other domains than continuous optimization.

We then showed how both our modeling and solving mechanisms are modular enough to be extended in order to handle additional concerns by proposing some modifications which allow to take in account the handling of uncertainties during the optimization process. We detailed the requirements for the agents to be able to handle uncertain values instead of normal numerical values, and we introduced the new concept of uncertainty propagator to provide, using the designers expertise of the uncertainties, an automatic mechanism for propagating heterogeneous uncertainties in the system.
\\
Uncertainties handling is an important concerns in design optimization. As design optimization problems are among the most complex continuous optimization problems, it seemed important to us to provide such functionalities for the system. A quite interesting work would be to propose new ways to extend the system, for example replacing numerical values by vectors, or by extending model agents to manipulate multiple models of different fidelity, experimenting with the possibilities (and the limits) of our proposed modeling.
